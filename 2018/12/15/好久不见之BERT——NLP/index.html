<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="不知不觉已经很久不写了……看了看之前的，还真觉得有的知识真是得记下来啊（因为脑子装不下……哈哈哈）回归主题，今天整理了一下nlp最新的BERT模型，涉及太多不懂的了，仍在studying……" />
  

  
  
  
  
  
  
  <title>好久不见之BERT——NLP | readyFly</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="不知不觉已经很久不写了……看了看之前的，还真觉得有的知识真是得记下来啊（因为脑子装不下……哈哈哈）回归主题，今天整理了一下nlp最新的BERT模型，涉及太多不懂的了，仍在studying……">
<meta property="og:type" content="article">
<meta property="og:title" content="好久不见之BERT——NLP">
<meta property="og:url" content="https://readyfly.github.io/2018/12/15/好久不见之BERT——NLP/index.html">
<meta property="og:site_name" content="readyFly">
<meta property="og:description" content="不知不觉已经很久不写了……看了看之前的，还真觉得有的知识真是得记下来啊（因为脑子装不下……哈哈哈）回归主题，今天整理了一下nlp最新的BERT模型，涉及太多不懂的了，仍在studying……">
<meta property="og:locale" content="Chinese">
<meta property="og:image" content="https://readyfly.github.io/upload_image/MLM.png">
<meta property="og:updated_time" content="2018-12-15T13:52:26.922Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="好久不见之BERT——NLP">
<meta name="twitter:description" content="不知不觉已经很久不写了……看了看之前的，还真觉得有的知识真是得记下来啊（因为脑子装不下……哈哈哈）回归主题，今天整理了一下nlp最新的BERT模型，涉及太多不懂的了，仍在studying……">
<meta name="twitter:image" content="https://readyfly.github.io/upload_image/MLM.png">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
  <link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
  <!--
  <script>
    (function(){
        if('{{ page.password }}'){
            if (prompt('请输入文章密码') == '{{ page.password }}'){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
-->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <a href="https://readyfly.github.io" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="readyFly" rel="home">readyFly</a>
      </h1>
      
        <script type="text/javascript" src="http://api.hitokoto.us/rand?encode=js&charset=utf-8"></script>
        <h2 class="site-description"><script>hitokoto();</script></h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">Archives</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-好久不见之BERT——NLP" class="post-好久不见之BERT——NLP post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      好久不见之BERT——NLP
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://readyfly.github.io/2018/12/15/好久不见之BERT——NLP/" data-id="cjppjgbvt000dpki2zv2coh2d" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>不知不觉已经很久不写了……看了看之前的，还真觉得有的知识真是得记下来啊（因为脑子装不下……哈哈哈）回归主题，今天整理了一下nlp最新的BERT模型，涉及太多不懂的了，仍在studying……<br><a id="more"></a></p>
<p>其实网上也有很多学习笔记（我就是这样看的……）此处只是自己的整理笔记</p>
<h2 id="首先先丢几个链接："><a href="#首先先丢几个链接：" class="headerlink" title="首先先丢几个链接："></a>首先先丢几个链接：</h2><p>从Word Embedding到Bert模型——自然语言处理预训练技术发展史<br><a href="https://mp.weixin.qq.com/s/FHDpx2cYYh9GZsa5nChi4g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/FHDpx2cYYh9GZsa5nChi4g</a></p>
<p>原码<br><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></p>
<p>BERT相关论文、文章和代码资源汇总<br><a href="http://www.52nlp.cn/tag/bert%E4%BB%A3%E7%A0%81" target="_blank" rel="noopener">http://www.52nlp.cn/tag/bert%E4%BB%A3%E7%A0%81</a><br>(看完这里面的也就差不多了……)</p>
<p>BERT论文讲解<br><a href="https://www.jiqizhixin.com/articles/2018-10-19-19" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-10-19-19</a></p>
<p>NLP基本处理流程<br><a href="https://www.jianshu.com/p/b87e01374a65" target="_blank" rel="noopener">https://www.jianshu.com/p/b87e01374a65</a></p>
<p>如果你对这些词不了解的话，需要先了解一下先…………（由于某些原因，之后再总结这方面内容……）</p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h2 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h2><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>与按顺序读取文本输入的方向模型（从左到右或从右到左）相反，Transformer编码器一次读取整个单词序列。因此它被认为是双向的，尽管说它是非定向的更准确。该特征允许模型基于其所有周围环境（单词的左侧和右侧）来学习单词的上下文。</p>
<p>有两种现有的将预先训练过的语言表示应用于下游任务的策略：基于特征的优化和微调。基于特征的 feature-based (比如  ELMo) and  基于参数微调的  fine-tuning (GPT)<br>这两种方法在训练前都有相同的目标函数，它们使用单向语言模型来学习通用语言表示。 </p>
<p>之后更新……</p>
<h2 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h2><p>在将单词序列输入BERT之前，每个序列中15％的单词被[MASK]标记替换。然后，该模型基于序列中其他未掩盖的单词提供的上下文，尝试预测被掩盖的单词的原始值。</p>
<p><img src="/upload_image/MLM.png" alt=""></p>
<p>图1：预训练模型架构的差异。BERT使用双向Transformer。OpenAI GPT使用从左到右的Transformer。ELMo使用经过独立训练的从左到右和从右到左LSTM的串联来生成下游任务的特征。三个模型中，只有BERT表示在所有层中共同依赖于左右上下文。</p>
<p>为了达到真正的bidirectional的LM的效果，作者创新性的提出了Masked LM，但是缺点是如果常常把一些词mask起来，未来的fine tuning过程中模型有可能没见过这些词。这个量积累下来还是很大的。因为作者在他的实现中随机选择了句子中15%的WordPiece tokens作为要mask的词。</p>
<p>为了解决这个问题，作者在做mask的时候：<br>1）80%的时间真的用[MASK]取代被选中的词。比如 my dog is hairy -&gt; my dog is [MASK]<br>2)  10%的时间用一个随机词取代它：my dog is hairy -&gt; my dog is apple<br>3)  10%的时间保持不变: my dog is hairy -&gt; my dog is hairy</p>
<p>为什么要以一定的概率保持不变呢？ 这是因为刚才说了，如果100%的时间都用[MASK]来取代被选中的词，那么在fine tuning的时候模型会有一些没见过的词。那么为啥要以一定的概率使用随机词呢？这是因为Transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是”hairy”。至于使用随机词带来的负面影响，文章中说了,所有其他的token(即非”hairy”的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。</p>
<h2 id="NSP-下一句话预测"><a href="#NSP-下一句话预测" class="headerlink" title="NSP  下一句话预测"></a>NSP  下一句话预测</h2><p>在BERT训练过程中，模型接收成对的句子作为输入，并学习预测该对中的第二个句子是否是原始文档中的后续句子。在训练期间，50％的输入是一对，其中第二句是原始文档中的后续句子，而在另外50％中，选择来自语料库的随机句作为第二句。假设随机句子将与第一句断开。</p>
<p>为了帮助模型区分训练中的两个句子，输入在进入模型之前按以下方式处理：<br>1、在第一个句子的开头插入[CLS]标记，并在每个句子的末尾插入[SEP]标记。<br>2、将表示句子A或句子B的句子嵌入添加到每个令牌。句子嵌入在概念上类似于2个词的词汇表的标记嵌入。<br>3、将位置嵌入添加到每个标记以指示其在序列中的位置。位置嵌入的概念和实现在Transformer论文中给出。</p>
<p>要预测第二个句子是否确实与第一个句子连接，执行以下步骤：<br>1、整个输入序列通过Transformer模型。<br>2、使用简单的分类层（学习好的重量和偏差矩阵）将[CLS]标记的输出变换为2×1大小的向量。<br>3、使用softmax计算IsNextSequence的概率。<br>在训练BERT模型时，Masked LM和Next Sentence Prediction被一起训练，目标是最小化两种策略的组合损失函数。</p>
<h2 id="如何使用BERT（微调）"><a href="#如何使用BERT（微调）" class="headerlink" title="如何使用BERT（微调）"></a>如何使用BERT（微调）</h2><p>将BERT用于特定任务相对简单：<br>BERT可用于各种语言任务，而只需在核心模型中添加一个小层：<br>1、通过在[CLS]令牌的Transformer输出之上添加分类层，类似于Next Sentence分类，进行情感分析等分类任务。<br>2、在问题回答任务（例如SQuAD v1.1）中，软件会收到有关文本序列的问题，并且需要在序列中标记答案。使用BERT，可以通过学习标记答案开始和结束的两个额外向量来训练Q＆A模型。<br>3、在命名实体识别（NER）中，软件接收文本序列，并且需要标记出现在文本中的各种类型的实体（人员，组织，日期等）。使用BERT，可以通过将每个令牌的输出向量馈送到预测NER标签的分类层来训练NER模型。</p>
<p>在微调训练中，大多数超参数保持与BERT训练相同，本文给出了需要调整的超参数的具体指导。BERT团队使用这种技术在各种具有挑战性的自然语言任务中获得最先进的结果。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>模型尺寸很重要，即使规模很大。BERT_large具有3.45亿个参数，是同类产品中最大的模型。它在小规模任务上显然优于BERT_base，后者使用相同的架构，只有”1.1亿”参数。</li>
<li>有足够的训练数据，更多的训练步骤==更高的准确性。例如，在MNLI任务上，训练1M级时，比500 K级获得了近1.0%的额外精度。</li>
<li>BERT的双向方法（MLM）收敛速度慢于从左到右的方法（因为每批中只预测了15％的单词），但是在少量预训练步骤之后，双向训练仍然优于从左到右的训练。</li>
</ol>
<h2 id="Softmax函数"><a href="#Softmax函数" class="headerlink" title="Softmax函数"></a>Softmax函数</h2><p>Softmax函数，或称归一化指数函数<br>他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。<br>最后的输出是每个分类被取到的概率。</p>
<p>有待更新……</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/12/15/好久不见之BERT——NLP/">
    <time datetime="2018-12-15T13:10:05.000Z" class="entry-date">
        2018-12-15
    </time>
</a>
    
    
    </footer>
</article>


  
  <!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTY4MC8xODIyNg==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
  




</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2018/12/15/好久不见之BERTRT问题篇/">好久不见之BERT问题篇</a>
          </li>
        
          <li>
            <a href="/2018/12/15/好久不见之BERT——NLP/">好久不见之BERT——NLP</a>
          </li>
        
          <li>
            <a href="/2018/07/19/数据结构之最小生成树/">数据结构之最小生成树</a>
          </li>
        
          <li>
            <a href="/2018/07/19/机器学习之K近邻/">机器学习之K近邻</a>
          </li>
        
          <li>
            <a href="/2018/07/19/操作系统复习/">操作系统复习</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-库函数/">C++ 库函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-means-无监督学习/">K-means 无监督学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K近邻/">K近邻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hello-world/">hello world</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo-博客/">hexo 博客</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-sklearn机器学习/">python sklearn机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前序、中序、后序遍历/">前序、中序、后序遍历</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前缀、中缀、后缀表达式/">前缀、中缀、后缀表达式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/动态规划/">动态规划</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/双向循环链表/">双向循环链表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/排序算法/">排序算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/最小生成树/">最小生成树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/最短路径算法-dijkstra算法-floyd算法/">最短路径算法 dijkstra算法 floyd算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机网络/">计算机网络</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贪心算法/">贪心算法</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/C-库函数/" style="font-size: 10px;">C++ 库函数</a> <a href="/tags/K-means-无监督学习/" style="font-size: 10px;">K-means 无监督学习</a> <a href="/tags/K近邻/" style="font-size: 10px;">K近邻</a> <a href="/tags/hello-world/" style="font-size: 10px;">hello world</a> <a href="/tags/hexo-博客/" style="font-size: 10px;">hexo 博客</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/python-sklearn机器学习/" style="font-size: 10px;">python sklearn机器学习</a> <a href="/tags/前序、中序、后序遍历/" style="font-size: 10px;">前序、中序、后序遍历</a> <a href="/tags/前缀、中缀、后缀表达式/" style="font-size: 10px;">前缀、中缀、后缀表达式</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/双向循环链表/" style="font-size: 10px;">双向循环链表</a> <a href="/tags/排序算法/" style="font-size: 10px;">排序算法</a> <a href="/tags/操作系统/" style="font-size: 10px;">操作系统</a> <a href="/tags/最小生成树/" style="font-size: 10px;">最小生成树</a> <a href="/tags/最短路径算法-dijkstra算法-floyd算法/" style="font-size: 10px;">最短路径算法 dijkstra算法 floyd算法</a> <a href="/tags/算法/" style="font-size: 20px;">算法</a> <a href="/tags/计算机网络/" style="font-size: 10px;">计算机网络</a> <a href="/tags/贪心算法/" style="font-size: 10px;">贪心算法</a>
    </div>
  </aside>

  
    <p class="asidetitle">打赏他</p>
<div>
<form action="https://shenghuo.alipay.com/send/payment/fill.htm" method="POST" target="_blank" accept-charset="GBK">
    <br/>
    <input name="optEmail" type="hidden" value="your 支付宝账号" />
    <input name="payAmount" type="hidden" value="默认捐赠金额(元)" />
    <input id="title" name="title" type="hidden" value="博主，打赏你的！" />
    <input name="memo" type="hidden" value="你Y加油，继续写博客！" />
    <input name="pay" type="image" value="转账" src="http://7xig3q.com1.z0.glb.clouddn.com/alipay-donate-website.png" />
</form>
</div>
  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2018 Feng_linhui
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次, 访客数 <span id="busuanzi_value_site_uv"></span> 人次, 本文总阅读量 <span id="busuanzi_value_page_pv"></span> 次

</footer>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/love.js"></script>
  <!--
 <embed src="//music.163.com/style/swf/widget.swf?sid=528283&type=2&auto=1&width=320&height=66" width="340" height="86"  allowNetworking="all"></embed>
 <img src = "/upload_image/cute.gif" width ="100" height="180" />
 -->
<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":50,"height":150},"mobile":{"show":true}});</script></body>
</html>