<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="整理的看BERT论文中的一些问题……有待更新" />
  

  
  
  
  
  
  
  <title>好久不见之BERT问题篇 | readyFly</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="整理的看BERT论文中的一些问题……有待更新">
<meta property="og:type" content="article">
<meta property="og:title" content="好久不见之BERT问题篇">
<meta property="og:url" content="https://readyfly.github.io/2018/12/15/好久不见之BERTRT问题篇/index.html">
<meta property="og:site_name" content="readyFly">
<meta property="og:description" content="整理的看BERT论文中的一些问题……有待更新">
<meta property="og:locale" content="Chinese">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert1.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert2.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert3.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert5.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert6.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert7.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert8.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert9.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert10.png">
<meta property="og:image" content="https://readyfly.github.io/upload_image/bert11.png">
<meta property="og:updated_time" content="2018-12-16T08:20:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="好久不见之BERT问题篇">
<meta name="twitter:description" content="整理的看BERT论文中的一些问题……有待更新">
<meta name="twitter:image" content="https://readyfly.github.io/upload_image/bert1.png">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
  <link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
  <!--
  <script>
    (function(){
        if('{{ page.password }}'){
            if (prompt('请输入文章密码') == '{{ page.password }}'){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>
-->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <a href="https://readyfly.github.io" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="readyFly" rel="home">readyFly</a>
      </h1>
      
        <script type="text/javascript" src="http://api.hitokoto.us/rand?encode=js&charset=utf-8"></script>
        <h2 class="site-description"><script>hitokoto();</script></h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">Archives</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-好久不见之BERTRT问题篇" class="post-好久不见之BERTRT问题篇 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      好久不见之BERT问题篇
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://readyfly.github.io/2018/12/15/好久不见之BERTRT问题篇/" data-id="cjzs70w3h001mzwvf6h4x41qu" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>整理的看BERT论文中的一些问题……有待更新<br><a id="more"></a></p>
<h2 id="1、编码问题-Bidirectional-Encoder-Representations-from-Transformers-只是知道它是利用双向学习-一次读取整个单词序列。不是很明白-transformer的encoder-、decoder"><a href="#1、编码问题-Bidirectional-Encoder-Representations-from-Transformers-只是知道它是利用双向学习-一次读取整个单词序列。不是很明白-transformer的encoder-、decoder" class="headerlink" title="1、编码问题  Bidirectional Encoder Representations from Transformers   只是知道它是利用双向学习  一次读取整个单词序列。不是很明白 transformer的encoder 、decoder"></a>1、编码问题  Bidirectional Encoder Representations from Transformers   只是知道它是利用双向学习  一次读取整个单词序列。不是很明白 transformer的encoder 、decoder</h2><p>答：可以看一下以下链接：<br><a href="https://blog.csdn.net/mijiaoxiaosan/article/details/73251443" target="_blank" rel="noopener">https://blog.csdn.net/mijiaoxiaosan/article/details/73251443</a><br><a href="https://www.codercto.com/a/26520.html" target="_blank" rel="noopener">https://www.codercto.com/a/26520.html</a><br><img src="/upload_image/bert1.png" alt=""></p>
<p>首先了解一下Transformer的作用。Transformer是一个自编码器，由encoder和decoder组成，可以用来做机器翻译。这个图是《attention is all you need》里面的图，图中的模型在机器翻译任务中应该是这样工作的。假设是将“我爱中国”翻译成“l love china”，那么encoder这里的input应该就是“我爱中国”，decoder这里的output应该是“l love china”。模型首先通过encoder将“我爱中国”编码成特征A，decoder结合输入“l love china”，将特征A解码为概率分布B。我理解的机器翻译的任务大概是这么训练的。</p>
<p>Encoder的作用主要是得到从中文句子中得到特征A，方法就是通过N层的  “由多头自注意力网络和前馈神经网络组成的”网络（图1红框）来提取特征。红框的网络下面部分是多头自注意力网络（图2），“多头”的意思是将一个大的向量划分成h个小的向量来计算（因为这样效果更好，我也不知道为啥会更好），“自注意力”是指将同一个向量作为V、K、Q的值输入。</p>
<p>Decoder作用主要是根据encoder得到地特征A，结合输入的output embadding来得到分布B。它的网络是图1的蓝色框，由两层多头注意力网络和一层前馈神经网络组成。最下面那层多头注意力网络的输入是output embadding，上面那层的Q,K的输入是encoder得到的特征A，输入V是下面那层注意力网络的输出。<br>Transformer在BERT中的应用时input 和output是一样的，输出的时候不用去计算分布，只需要得到最后一个网络的结果就行，图一绿色箭头部分。</p>
<p>附上transform学习：<br><a href="https://baijiahao.baidu.com/s?id=1614828439463695390&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1614828439463695390&amp;wfr=spider&amp;for=pc</a></p>
<p>需要注意的是：BERT并不完全是Decoder and  encoder  , 实际上只有编码器encoder （这是一个新问题……待解决）</p>
<h2 id="2、ELMo和MLM的理解"><a href="#2、ELMo和MLM的理解" class="headerlink" title="2、ELMo和MLM的理解"></a>2、ELMo和MLM的理解</h2><p>ELMo也是双向的 但是是对 两个方向的 一个线性组合 那么MLM和他最大的区别是将两个方向的信息同时利用而不是分别提取后再组合，这样理解对吗？<br>参考词向量技术：<a href="https://www.jianshu.com/p/a6bc14323d77" target="_blank" rel="noopener">https://www.jianshu.com/p/a6bc14323d77</a></p>
<p>答：<br>ELMo是从左到右的LSTM和从右到左的LSTM网络训练得到的embadding拼接起来作为下游任务的输入，以这样的方法来得到单词的上下文信息。个人感觉这样做的话，确实是利用了上下文信息，但是感觉用的有点生硬。<br>MLM任务：随机遮住15%的单词，预测这些单词，相当于做完形填空。我认为这样做能够更准确地涵盖上下文的意思。所以在BERT这篇文章里面将MLM这样的embadding方式称为深层双向的模型。</p>
<h2 id="3、mask一部分，因为在其他任务中不能mask掉这些词，所以采用了80-加mask-10-用随机词替换-10-保留原词-要msak的那个位置是定好了的吗-整个这个过程不太清楚？怎么计算预测的准确性呢？"><a href="#3、mask一部分，因为在其他任务中不能mask掉这些词，所以采用了80-加mask-10-用随机词替换-10-保留原词-要msak的那个位置是定好了的吗-整个这个过程不太清楚？怎么计算预测的准确性呢？" class="headerlink" title="3、mask一部分，因为在其他任务中不能mask掉这些词，所以采用了80%加mask   10%用随机词替换  10%保留原词    要msak的那个位置是定好了的吗  整个这个过程不太清楚？怎么计算预测的准确性呢？"></a>3、mask一部分，因为在其他任务中不能mask掉这些词，所以采用了80%加mask   10%用随机词替换  10%保留原词    要msak的那个位置是定好了的吗  整个这个过程不太清楚？怎么计算预测的准确性呢？</h2><p><img src="/upload_image/bert2.png" alt=""></p>
<p>答：<br>文中的解释：Mask的位置是随机的。即对于一段文字，随机mask其中15%的词，由于encoder在处理的时候，不知道哪些词是被遮住了，所以它会被迫保持每一个单词的上下文表示。</p>
<p>MLM任务是随机遮住单词，然后通过学习来预测它本身，相当于完形填空。为了预测mask这个词，encoder会对关注mask这个词的上下文。由于一开始输入的时候，我们是随机mask单词的，encoder不知道哪些单词被mask了，所以他会对每个单词的上下文都关注。（这个关注是什么呢？）<br>对上下文的关注是指，对这个词进行embadding时，包含了上下文的信息</p>
<p>Transformer编码器不知道它将被要求预测哪些单词，或者哪些已经被随机单词替换，因此它必须对每个输入词保持分布式的上下文表示？？？</p>
<p>怎么计算预测的准确性:<br>将预测的词语原来的词相比较。</p>
<p><img src="/upload_image/bert3.png" alt=""></p>
<p>可以理解为：在随机选择mask的时候原来的词是会被记录的，然后与预测出来的词做比较吗？（可以，看源代码就知道了）</p>
<h2 id="4、MLM算是无监督吗-，预测下一句NSP-有标注lable，那它就是监督学习？"><a href="#4、MLM算是无监督吗-，预测下一句NSP-有标注lable，那它就是监督学习？" class="headerlink" title="4、MLM算是无监督吗 ，预测下一句NSP 有标注lable，那它就是监督学习？"></a>4、MLM算是无监督吗 ，预测下一句NSP 有标注lable，那它就是监督学习？</h2><p>答：<br>我觉得MLM是有监督的，label就是被遮住的那个词。这一点我之前也没有想过，我是在上一题中看了代码才这样认为的。<br>NSP是有监督的。(有点疑问……)</p>
<h2 id="5、在训练BERT模型时，Masked-LM和Next-Sentence-Prediction被一起训练，目标是最小化两种策略的组合损失函数。没懂？"><a href="#5、在训练BERT模型时，Masked-LM和Next-Sentence-Prediction被一起训练，目标是最小化两种策略的组合损失函数。没懂？" class="headerlink" title="5、在训练BERT模型时，Masked LM和Next Sentence Prediction被一起训练，目标是最小化两种策略的组合损失函数。没懂？"></a>5、在训练BERT模型时，Masked LM和Next Sentence Prediction被一起训练，目标是最小化两种策略的组合损失函数。没懂？</h2><p>答：<br>我的理解：训练bert模型的过程主要就是将输入的embaddings Ei经过一系列操作变成输出的Ti。如下图：</p>
<p><img src="/upload_image/bert5.png" alt=""></p>
<p>这个图的是对  一对句子进行分类，（NSP）其实也是这样的任务。对于这样的任务，主要就是对calss label进行分类。对于MLM任务，其实就是在最后一层对被mask的T（红框框里的词）进行预测。所以说这两个任务是同时进行的。</p>
<p>对“目标是最小化两种策略的组合损失函数”的理解，可以看代码：</p>
<p><img src="/upload_image/bert6.png" alt=""></p>
<p>可以看出，其实最终的loss就是两个子任务的loss的和。</p>
<h2 id="6、输入都是一个个的token-但论文中是英文的，如果是中文的词-在数据清洗之后或者去掉一些停用词之类的-会不会有什么影响呢？"><a href="#6、输入都是一个个的token-但论文中是英文的，如果是中文的词-在数据清洗之后或者去掉一些停用词之类的-会不会有什么影响呢？" class="headerlink" title="6、输入都是一个个的token 但论文中是英文的，如果是中文的词  在数据清洗之后或者去掉一些停用词之类的 会不会有什么影响呢？"></a>6、输入都是一个个的token 但论文中是英文的，如果是中文的词  在数据清洗之后或者去掉一些停用词之类的 会不会有什么影响呢？</h2><p><img src="/upload_image/bert7.png" alt=""></p>
<p>这个图没太明白……目前的理解是：第一层是对于token 的编码    第二层是对不同语句的编码  第三层是对每个token位置的一个编码  但是这三个编码具体怎么利用的呢？</p>
<p>答：<br>关于处理中文是，在去掉停用词之后，会不会对BERT模型有影响。猜想一下，如果这些操作没有改变语义的话，应该是没有影响的。关于处理汉语的BERT模型，官方在github上面以及给出训练好的模型，如果需要的话可以去下载。<br><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a><br>对于句子中的每一个词，例如my这个单词，BERT模型是将token embadding、segment embadding、position embadding这三个向量相加之后得到的向量作为最终的输入embadding。</p>
<h2 id="7、没太懂这个收敛是什么意思"><a href="#7、没太懂这个收敛是什么意思" class="headerlink" title="7、没太懂这个收敛是什么意思 ?"></a>7、没太懂这个收敛是什么意思 ?</h2><p>答：<br><img src="/upload_image/bert8.png" alt=""></p>
<p>因为BERT模型在每一次训练过程中只对当前batch中的15%的词进行预测（因为每次随机遮住15%的词），而LTR模型在训练的时候每一次都会对每一次进行训练。所以用MLM的话，收敛速度会比LTR慢。直观上也可以理解，假设一个sequence有100个token，那么在BERT模型的MLM任务中，每次只有随机15个单词被预测，所以需要多训练几次，这儿样随机到的词就更多，更有利于上下文的表达。另外预测这15个词的过程也会有时间上的开销，所以收敛会慢一点。</p>
<p>BERT损失函数仅考虑掩盖值的预测并忽略非掩盖字的预测。因此，模型比定向模型收敛得慢</p>
<h2 id="8、模型具体的参数设置还是不太明白。fine-tuning微调？"><a href="#8、模型具体的参数设置还是不太明白。fine-tuning微调？" class="headerlink" title="8、模型具体的参数设置还是不太明白。fine-tuning微调？"></a>8、模型具体的参数设置还是不太明白。fine-tuning微调？</h2><p>答：</p>
<p>BERT BASE:  L = 12,H = 768,A = 12<br>L：Num of layers，下图中红色部分，含义是encoder和decoder的各自网络层数<br>H:  Hidden size，蓝色部分，隐藏层<br>A:  Num of Heads，绿色部分，多头注意力机制中头的部分</p>
<p><img src="/upload_image/bert9.png" alt=""></p>
<p>在微调过程中，参数要加载预训练时候的参数：</p>
<p><img src="/upload_image/bert10.png" alt=""></p>
<p>附上模型参数解释：</p>
<pre><code>Args:
  vocab_size: 输入BERT模型的词汇大小
  hidden_size: 编码器层和较小层的大小
  num_hidden_layers: Number of hidden layers in the Transformer encoder.
  num_attention_heads: Number of attention heads for each attention layer in
    the Transformer encoder.
  intermediate_size: The size of the &quot;intermediate&quot; (i.e., feed-forward)
    layer in the Transformer encoder.
  hidden_act: The non-linear activation function (function or string) in the
    encoder and pooler.
  hidden_dropout_prob: The dropout probability for all fully connected
    layers in the embeddings, encoder, and pooler.
  attention_probs_dropout_prob: The dropout ratio for the attention
    probabilities.
  max_position_embeddings: The maximum sequence length that this model might
    ever be used with. Typically set this to something large just in case
    (e.g., 512 or 1024 or 2048).
  type_vocab_size: The vocabulary size of the `token_type_ids` passed into
    `BertModel`.
  initializer_range: The stdev of the truncated_normal_initializer for
    initializing all weight matrices.
&quot;&quot;&quot;
</code></pre><p>如果要跑BERT的示例程序，那么执行命令中需要注意调一下以下参数：</p>
<p><img src="/upload_image/bert11.png" alt=""></p>
<p>这几个主要是对于建立在BERT预训练之后的网络</p>
<p>数据集：GULE</p>
<p>PS：解答来自某位师兄and网上，仅供参考！</p>
<p>待完善…………</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/12/15/好久不见之BERTRT问题篇/">
    <time datetime="2018-12-15T13:22:52.000Z" class="entry-date">
        2018-12-15
    </time>
</a>
    
    
    </footer>
</article>


  
  <!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC80MTY4MC8xODIyNg==">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
  




</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/08/26/实战篇——达观杯比赛，信息抽取/">实战篇——达观杯比赛，信息抽取</a>
          </li>
        
          <li>
            <a href="/2019/08/26/主动学习/">主动学习</a>
          </li>
        
          <li>
            <a href="/2019/08/26/论文-BERT-翻译/">论文:BERT 翻译</a>
          </li>
        
          <li>
            <a href="/2019/08/26/nlp/">nlp</a>
          </li>
        
          <li>
            <a href="/2019/08/21/NLP自然语言处理/">NLP自然语言处理</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-库函数/">C++ 库函数</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K-means-无监督学习/">K-means 无监督学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/K近邻/">K近邻</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hello-world/">hello world</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo-博客/">hexo 博客</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python-sklearn机器学习/">python sklearn机器学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前序、中序、后序遍历/">前序、中序、后序遍历</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/前缀、中缀、后缀表达式/">前缀、中缀、后缀表达式</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/动态规划/">动态规划</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/双向循环链表/">双向循环链表</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/排序算法/">排序算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/最小生成树/">最小生成树</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/最短路径算法-dijkstra算法-floyd算法/">最短路径算法 dijkstra算法 floyd算法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/计算机网络/">计算机网络</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/贪心算法/">贪心算法</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/C-库函数/" style="font-size: 10px;">C++ 库函数</a> <a href="/tags/K-means-无监督学习/" style="font-size: 10px;">K-means 无监督学习</a> <a href="/tags/K近邻/" style="font-size: 10px;">K近邻</a> <a href="/tags/hello-world/" style="font-size: 10px;">hello world</a> <a href="/tags/hexo-博客/" style="font-size: 10px;">hexo 博客</a> <a href="/tags/mysql/" style="font-size: 10px;">mysql</a> <a href="/tags/python-sklearn机器学习/" style="font-size: 10px;">python sklearn机器学习</a> <a href="/tags/前序、中序、后序遍历/" style="font-size: 10px;">前序、中序、后序遍历</a> <a href="/tags/前缀、中缀、后缀表达式/" style="font-size: 10px;">前缀、中缀、后缀表达式</a> <a href="/tags/动态规划/" style="font-size: 10px;">动态规划</a> <a href="/tags/双向循环链表/" style="font-size: 10px;">双向循环链表</a> <a href="/tags/排序算法/" style="font-size: 10px;">排序算法</a> <a href="/tags/操作系统/" style="font-size: 10px;">操作系统</a> <a href="/tags/最小生成树/" style="font-size: 10px;">最小生成树</a> <a href="/tags/最短路径算法-dijkstra算法-floyd算法/" style="font-size: 10px;">最短路径算法 dijkstra算法 floyd算法</a> <a href="/tags/算法/" style="font-size: 20px;">算法</a> <a href="/tags/计算机网络/" style="font-size: 10px;">计算机网络</a> <a href="/tags/贪心算法/" style="font-size: 10px;">贪心算法</a>
    </div>
  </aside>

  
    <p class="asidetitle">打赏他</p>
<div>
<form action="https://shenghuo.alipay.com/send/payment/fill.htm" method="POST" target="_blank" accept-charset="GBK">
    <br/>
    <input name="optEmail" type="hidden" value="your 支付宝账号" />
    <input name="payAmount" type="hidden" value="默认捐赠金额(元)" />
    <input id="title" name="title" type="hidden" value="博主，打赏你的！" />
    <input name="memo" type="hidden" value="你Y加油，继续写博客！" />
    <input name="pay" type="image" value="转账" src="http://7xig3q.com1.z0.glb.clouddn.com/alipay-donate-website.png" />
</form>
</div>
  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Feng_linhui
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次, 访客数 <span id="busuanzi_value_site_uv"></span> 人次, 本文总阅读量 <span id="busuanzi_value_page_pv"></span> 次

</footer>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/love.js"></script>
  <!--
 <embed src="//music.163.com/style/swf/widget.swf?sid=528283&type=2&auto=1&width=320&height=66" width="340" height="86"  allowNetworking="all"></embed>
 <img src = "/upload_image/cute.gif" width ="100" height="180" />
 -->
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":50,"height":150},"mobile":{"show":true}});</script></body>
</html>